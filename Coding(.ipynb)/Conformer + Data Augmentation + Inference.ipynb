{"cells":[{"cell_type":"markdown","metadata":{},"source":["The model consists of a Lankmark Embbeding + Conformer.\n","\n","Data Augmentation was applied.\n","\n","Inference is performed by starting with an SOS token and predicting one character at a time using the previous prediction.\n","\n","Inference requires the encoder to encode the input frames and subsequently use that encoding to predict the 1st character by inputting the encoding and SOS (Start of Sentence) token. Next, the encoding, SOS token and 1st predicted token are used to predict the 2nd character. Inference thus requires 1 call to the encoder and multiple calls to the encoder. On average a phrase is 18 characters long, requiring 18+1(SOS token) calls to the decoder.\n","\n","Some inspiration is taken from the [1st place solution - training](https://www.kaggle.com/code/hoyso48/1st-place-solution-training) from the last [Google - Isolated Sign Language Recognition\n","](https://www.kaggle.com/competitions/asl-signs) competition.\n","\n","Special thanks for all of these guys, Many many thanks to them: \n","\n","https://www.kaggle.com/competitions/asl-fingerspelling/discussion/434364\n","\n","[1st place solution] Improved Squeezeformer + TransformerDecoder + Clever augmentations: https://www.kaggle.com/competitions/asl-fingerspelling/discussion/434485\n","\n","[5th place solution] Vanilla Transformer, Data2vec Pretraining, CutMix, and KD: https://www.kaggle.com/competitions/asl-fingerspelling/discussion/434415\n","\n","https://www.kaggle.com/code/gusthema/asl-fingerspelling-recognition-w-tensorflow\n","\n","This man helps me alot: https://www.kaggle.com/competitions/asl-fingerspelling/discussion/411060"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install neptune seaborn leven"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if 'tpu_init' not in globals():\n","    import tensorflow as tf\n","    import numpy as np\n","    import pandas as pd\n","    import matplotlib.pyplot as plt\n","    import matplotlib as mpl\n","    import seaborn as sn\n","    import tensorflow_addons as tfa\n","    from tensorflow import keras\n","    from tensorflow.keras import layers\n","\n","    from tqdm.notebook import tqdm\n","    from sklearn.model_selection import train_test_split, GroupShuffleSplit\n","    from leven import levenshtein\n","        \n","    import glob\n","    import os\n","    import math\n","    import gc\n","    import sys\n","    import sklearn\n","    import time\n","    import json\n","\n","    # TQDM Progress Bar With Pandas Apply Function\n","    tqdm.pandas()\n","\n","print(f'Tensorflow Version {tf.__version__}')\n","print(f'Python Version: {sys.version}')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define tand get the number os devices. \n","strategy = tf.distribute.MirroredStrategy()\n","print('DEVICES AVAILABLE: {}'.format(strategy.num_replicas_in_sync))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["TPU = True\n","VAL = 0\n","LOAD = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if 'tpu_init' not in globals():\n","    tpu_init = True\n","    if TPU:\n","        tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n","        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","    else:\n","        ngpu = len(tf.config.experimental.list_physical_devices('GPU'))\n","        if ngpu>1:\n","            print(\"Using multi GPU\")\n","            strategy = tf.distribute.MirroredStrategy()\n","        elif ngpu==1:\n","            print(\"Using single GPU\")\n","            strategy = tf.distribute.get_strategy()\n","            BATCH_SIZE = 64\n","        else:\n","            print(\"Using CPU\")\n","            strategy = tf.distribute.get_strategy()\n","            BATCH_SIZE = 2 "]},{"cell_type":"markdown","metadata":{},"source":["# Character 2 Ordinal Encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Read Character to Ordinal Encoding Mapping\n","with open('/kaggle/input/asl-fingerspelling/character_to_prediction_index.json') as json_file:\n","    CHAR2ORD = json.load(json_file)\n","    \n","# Ordinal to Character Mapping\n","ORD2CHAR = {j:i for i,j in CHAR2ORD.items()}\n","    \n","# Character to Ordinal Encoding Mapping   \n","display(pd.Series(CHAR2ORD).to_frame('Ordinal Encoding'))"]},{"cell_type":"markdown","metadata":{},"source":["# Global Config"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# If Notebook Is Run By Committing or In Interactive Mode For Development\n","IS_INTERACTIVE = os.environ['KAGGLE_KERNEL_RUN_TYPE'] == 'Interactive'\n","# Verbose Setting during training\n","VERBOSE = 1 if IS_INTERACTIVE else 2\n","# Global debug flag, takes subset of train\n","DEBUG = False\n","# Number of Unique Characters To Predict + Pad Token + SOS Token + EOS Token\n","N_UNIQUE_CHARACTERS0 = len(CHAR2ORD)\n","N_UNIQUE_CHARACTERS = len(CHAR2ORD) + 1 + 1 + 1\n","PAD_TOKEN = len(CHAR2ORD) # Padding\n","SOS_TOKEN = len(CHAR2ORD) + 1 # Start Of Sentence\n","EOS_TOKEN = len(CHAR2ORD) + 2 # End Of Sentence\n","# Whether to use 10% of data for validation\n","VAL = True\n","# Batch Size\n","BATCH_SIZE = 64\n","# Increase batch size if the TPU is available\n","if TPU:\n","    BATCH_SIZE = 25 * strategy.num_replicas_in_sync\n","    print(BATCH_SIZE)\n","\n","# Number of Epochs to Train for\n","N_EPOCHS = 100\n","\n","# Increase number of epochs if the Multi GPUs are available\n","if strategy.num_replicas_in_sync==2:\n","    N_EPOCHS = 400\n","    print(\"cuz 2 GPUs are available, so the number of epochs changes from 100 to : \", N_EPOCHS)\n","\n","# Increase number of epochs if the TPU is available\n","if TPU:\n","    N_EPOCHS = 500\n","    print(\"cuz tpu is available, number of epochs would be:\", N_EPOCHS)\n","\n","# Number of Warmup Epochs in Learning Rate Scheduler\n","N_WARMUP_EPOCHS = 10\n","# Maximum Learning Rate\n","LR_MAX = 1e-3\n","# Weight Decay Ratio as Ratio of Learning Rate\n","WD_RATIO = 0.05\n","# Length of Phrase + EOS Token\n","MAX_PHRASE_LENGTH = 31 + 1\n","# Whether to Train The model\n","TRAIN_MODEL = True\n","# Whether to Load Pretrained Weights\n","LOAD_WEIGHTS = False\n","# Learning Rate Warmup Method [log,exp]\n","WARMUP_METHOD = 'exp'\n","# Computing type\n","COMPUTE_TYPE = 'float32'\n","# Include Z axis\n","USE_Z_AXIS = True\n","# Number of Frames to resize recording to\n","FRAME_LEN = 384  \n","\n","# Global Random Seed\n","tf.random.set_seed(SEED)\n","np.random.seed(SEED)\n","# DROP_OUT ratio\n","DROP_OUT = 0.1\n","# Apply Masking\n","MASKING = True"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import neptune\n","\n","run = neptune.init_run(\n","    project=\"ASL-/ASL\",\n","    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI1NWFkNWMwNi00MTMzLTRiZGMtYjIwZi1jNGI0ZTU1MDVjNDYifQ==\",\n",")  "]},{"cell_type":"markdown","metadata":{},"source":["# Plot Config"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# MatplotLib Global Settings\n","mpl.rcParams.update(mpl.rcParamsDefault)\n","mpl.rcParams['xtick.labelsize'] = 16\n","mpl.rcParams['ytick.labelsize'] = 16\n","mpl.rcParams['axes.labelsize'] = 18\n","mpl.rcParams['axes.titlesize'] = 24"]},{"cell_type":"markdown","metadata":{},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Read Train DataFrame\n","if DEBUG:\n","    train = pd.read_csv('/kaggle/input/asl-fingerspelling/train.csv').head(5000)\n","else:\n","    train = pd.read_csv('/kaggle/input/asl-fingerspelling/train.csv')\n","    \n","# Set Train Indexed By sqeuence_id\n","train_sequence_id = train.set_index('sequence_id')\n","\n","# Number Of Train Samples\n","N_SAMPLES = len(train)\n","print(f'N_SAMPLES: {N_SAMPLES}')\n","\n","display(train.info())\n","display(train.head())"]},{"cell_type":"markdown","metadata":{},"source":["# File Path"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Get complete file path to file\n","def get_file_path(path):\n","    return f'/kaggle/input/asl-fingerspelling/{path}'\n","\n","train['file_path'] = train['path'].apply(get_file_path)"]},{"cell_type":"markdown","metadata":{},"source":["# Example File Paths"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Unique Parquet Files\n","INFERENCE_FILE_PATHS = pd.Series(\n","        glob.glob('/kaggle/input/aslfr-preprocessing-dataset/train_landmark_subsets/*')\n","    )\n","\n","print(f'Found {len(INFERENCE_FILE_PATHS)} Inference Pickle Files')"]},{"cell_type":"markdown","metadata":{},"source":["# Example Parquet"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Read First Parquet File\n","example_parquet_df_ = pd.read_parquet(train['file_path'][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["example_parquet_df = pd.read_parquet(INFERENCE_FILE_PATHS[0])\n","\n","# Each parquet file contains 1000 recordings\n","print(f'# Unique Recording: {example_parquet_df.index.nunique()}')\n","# Display DataFrame layout\n","display(example_parquet_df.head())"]},{"cell_type":"markdown","metadata":{},"source":["# Landmark Indices"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["NOSE=[\n","    1,2,98,327\n","]\n","LNOSE = [98]\n","RNOSE = [327]\n","LIP = [ 0, \n","    61, 185, 40, 39, 37, 267, 269, 270, 409,\n","    291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n","    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n","    95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n","]\n","LLIP = [84,181,91,146,61,185,40,39,37,87,178,88,95,78,191,80,81,82]\n","RLIP = [314,405,321,375,291,409,270,269,267,317,402,318,324,308,415,310,311,312]\n","\n","POSE = [500,501,502,503,504,505,506,507,508,509,510,511,512,513]\n","LPOSE = [513,505,503,501]\n","RPOSE = [512,504,502,500]\n","\n","REYE = [\n","    33, 7, 163, 144, 145, 153, 154, 155, 133,\n","    246, 161, 160, 159, 158, 157, 173,\n","]\n","LEYE = [\n","    263, 249, 390, 373, 374, 380, 381, 382, 362,\n","    466, 388, 387, 386, 385, 384, 398,\n","]\n","\n","LHAND = np.arange(468, 489).tolist()\n","RHAND = np.arange(522, 543).tolist()\n","\n","POINT_LANDMARKS = LIP + LHAND + RHAND + NOSE + REYE + LEYE + POSE\n","len(POINT_LANDMARKS)"]},{"cell_type":"markdown","metadata":{},"source":["#  Get Column by landmark Indices"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["get_cols=example_parquet_df_.columns\n","\n","def idx_to_cols(idx_array):\n","    sample_cols = [get_cols[i+1] for i in idx_array]\n","    # sample_cols = data.iloc[:,[i+1 for i in idx_array]].columns\n","    \n","    patterns = [s[1:] for s in sample_cols]\n","    if USE_Z_AXIS:\n","        matched_cols = [s for s in get_cols if s[1:] in patterns]\n","    else:\n","        matched_cols = [s for s in get_cols if s[1:] in patterns and s[0] != 'z']\n","    return matched_cols\n","\n","SEL_COLS = idx_to_cols(POINT_LANDMARKS)\n","print(len(SEL_COLS))\n","print(len(set(SEL_COLS)))\n","def get_index(arr):\n","    cols = idx_to_cols(arr)\n","    idx = [SEL_COLS.index(c) for c in cols]\n","    return idx"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["DATA_DIM = 3 if USE_Z_AXIS else 2\n","HAND_NUMS = len(LHAND)+ len(RHAND)\n","FACE_NUMS = len(LIP) + len(REYE) + len(LEYE) + len(NOSE)\n","POSE_NUMS = len(POSE)\n","print(HAND_NUMS, FACE_NUMS, POSE_NUMS)\n","print(HAND_NUMS+FACE_NUMS+POSE_NUMS)\n","\n","LIP_IDX = get_index(LIP)\n","LHAND_IDX = get_index(LHAND)\n","RHAND_IDX = get_index(RHAND)\n","NOSE_IDX = get_index(NOSE)\n","REYE_IDX = get_index(REYE)\n","LEYE_IDX = get_index(LEYE)\n","\n","LLIP_IDX = get_index(LLIP)\n","RLIP_IDX = get_index(RLIP)\n","LNOSE_IDX = get_index(LNOSE)\n","RNOSE_IDX = get_index(RNOSE)\n","\n","POSE_IDX = get_index(POSE)\n","LPOSE_IDX = get_index(LPOSE)\n","RPOSE_IDX = get_index(RPOSE)"]},{"cell_type":"markdown","metadata":{},"source":["# Data Preprocessing and Augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["NAN_FILL_VALUE = tf.constant(0, dtype=tf.float32)\n","NAN_VALUE = tf.constant(np.nan, dtype=tf.float32)\n","PADDING_MASKING_VALUE = tf.constant(-100, dtype=tf.float32)\n","\n","def resize_pad(x):\n","    if tf.shape(x)[0] < FRAME_LEN:\n","        if MASKING:\n","            if tf.shape(x)[0] < 32:\n","                x = tf.pad(\n","                    x,\n","                    ([[0, (32 - tf.shape(x)[0])], [0, 0], [0, 0]]),\n","                    constant_values = PADDING_MASKING_VALUE,\n","                )\n","        x = tf.pad(\n","            x,\n","            ([[0, (FRAME_LEN - tf.shape(x)[0])], [0, 0], [0, 0]]),\n","            constant_values = NAN_FILL_VALUE,\n","        )\n","    else:\n","        x = tf.image.resize(x, (FRAME_LEN, tf.shape(x)[1]), \"nearest\")\n","    return x\n","\n","def tf_nan_mean(x, axis=0, keepdims=False):\n","    return tf.reduce_sum(\n","        tf.where(tf.math.is_nan(x), tf.zeros_like(x), x), axis=axis, keepdims=keepdims\n","    ) / tf.reduce_sum(\n","        tf.where(tf.math.is_nan(x), tf.zeros_like(x), tf.ones_like(x)),\n","        axis=axis,\n","        keepdims=keepdims,\n","    )\n","\n","\n","def tf_nan_std(x, center=None, axis=0, keepdims=False):\n","    if center is None:\n","        center = tf_nan_mean(x, axis=axis, keepdims=True)\n","    d = x - center\n","    return tf.math.sqrt(tf_nan_mean(d * d, axis=axis, keepdims=keepdims))\n","\n","\n","def self_norm(x):\n","    # input batch, 21 + 39 + 33, 2\n","    mean_no_nan = tf_nan_mean(x, axis=[0,1],keepdims=True)\n","    std_no_nan = tf_nan_std(x, center=mean_no_nan, axis=[0,1],keepdims=True)\n","    x = (x - mean_no_nan) / (std_no_nan)\n","    return x\n","\n","def global_norm(x):\n","    # face = x[:,:len(LIP_IDX),:]\n","    pose = x[:,-len(POSE_IDX):,:]\n","    mean_no_nan = tf_nan_mean(pose, axis=[0,1],keepdims=True)\n","    std_no_nan = tf_nan_std(x, center=mean_no_nan, axis=[0,1],keepdims=True)\n","    x = (x - mean_no_nan) / (std_no_nan) / 3\n","    return x\n","\n","\n","def split_data(x):\n","    # POINT_LANDMARKS = LIP + LHAND + RHAND + NOSE + REYE + LEYE\n","\n","    lip = tf.gather(x, LIP_IDX, axis=1)\n","    if USE_Z_AXIS:\n","        lip_x = lip[:, 0 * (len(LIP_IDX) // 3) : 1 * (len(LIP_IDX) // 3)]\n","        lip_y = lip[:, 1 * (len(LIP_IDX) // 3) : 2 * (len(LIP_IDX) // 3)]\n","        lip_z = lip[:, 2 * (len(LIP_IDX) // 3) : 3 * (len(LIP_IDX) // 3)]\n","        lip = tf.concat(\n","            [lip_x[..., tf.newaxis], lip_y[..., tf.newaxis], lip_z[..., tf.newaxis]],\n","            axis=-1,\n","        )\n","    else:\n","        lip_x = lip[:, 0 * (len(LIP_IDX) // 2) : 1 * (len(LIP_IDX) // 2)]\n","        lip_y = lip[:, 1 * (len(LIP_IDX) // 2) : 2 * (len(LIP_IDX) // 2)]\n","\n","    lhand = tf.gather(x, LHAND_IDX, axis=1)\n","    rhand = tf.gather(x, RHAND_IDX, axis=1)\n","    if USE_Z_AXIS:\n","        lhand_x = lhand[:, 0 * (len(LHAND_IDX) // 3) : 1 * (len(LHAND_IDX) // 3)]\n","        lhand_y = lhand[:, 1 * (len(LHAND_IDX) // 3) : 2 * (len(LHAND_IDX) // 3)]\n","        lhand_z = lhand[:, 2 * (len(LHAND_IDX) // 3) : 3 * (len(LHAND_IDX) // 3)]\n","        lhand = tf.concat(\n","            [lhand_x[..., tf.newaxis], lhand_y[..., tf.newaxis], lhand_z[..., tf.newaxis]],\n","            axis=-1,\n","        )\n","        rhand_x = rhand[:, 0 * (len(RHAND_IDX) // 3) : 1 * (len(RHAND_IDX) // 3)]\n","        rhand_y = rhand[:, 1 * (len(RHAND_IDX) // 3) : 2 * (len(RHAND_IDX) // 3)]\n","        rhand_z = rhand[:, 2 * (len(RHAND_IDX) // 3) : 3 * (len(RHAND_IDX) // 3)]\n","        rhand = tf.concat(\n","            [rhand_x[..., tf.newaxis], rhand_y[..., tf.newaxis], rhand_z[..., tf.newaxis]],\n","            axis=-1,\n","        )\n","    else:\n","        lhand_x = lhand[:, 0 * (len(LHAND_IDX) // 2) : 1 * (len(LHAND_IDX) // 2)]\n","        lhand_y = lhand[:, 1 * (len(LHAND_IDX) // 2) : 2 * (len(LHAND_IDX) // 2)]\n","        rhand_x = rhand[:, 0 * (len(RHAND_IDX) // 2) : 1 * (len(RHAND_IDX) // 2)]\n","        rhand_y = rhand[:, 1 * (len(RHAND_IDX) // 2) : 2 * (len(RHAND_IDX) // 2)]\n","        lhand = tf.concat([lhand_x[..., tf.newaxis], lhand_y[..., tf.newaxis]], axis=-1)\n","        rhand = tf.concat([rhand_x[..., tf.newaxis], rhand_y[..., tf.newaxis]], axis=-1)\n","\n","    nose = tf.gather(x, NOSE_IDX, axis=1)\n","    if USE_Z_AXIS:\n","        nose_x = nose[:, 0 * (len(NOSE_IDX) // 3) : 1 * (len(NOSE_IDX) // 3)]\n","        nose_y = nose[:, 1 * (len(NOSE_IDX) // 3) : 2 * (len(NOSE_IDX) // 3)]\n","        nose_z = nose[:, 2 * (len(NOSE_IDX) // 3) : 3 * (len(NOSE_IDX) // 3)]\n","        nose = tf.concat(\n","            [nose_x[..., tf.newaxis], nose_y[..., tf.newaxis], nose_z[..., tf.newaxis]],axis=-1)\n","    else:\n","        nose_x = nose[:, 0 * (len(NOSE_IDX) // 2) : 1 * (len(NOSE_IDX) // 2)]\n","        nose_y = nose[:, 1 * (len(NOSE_IDX) // 2) : 2 * (len(NOSE_IDX) // 2)]\n","        nose = tf.concat([nose_x[..., tf.newaxis], nose_y[..., tf.newaxis]], axis=-1)\n","\n","    reye = tf.gather(x, REYE_IDX, axis=1)\n","    leye = tf.gather(x, LEYE_IDX, axis=1)\n","    if USE_Z_AXIS:\n","        reye_x = reye[:, 0 * (len(REYE_IDX) // 3) : 1 * (len(REYE_IDX) // 3)]\n","        reye_y = reye[:, 1 * (len(REYE_IDX) // 3) : 2 * (len(REYE_IDX) // 3)]\n","        reye_z = reye[:, 2 * (len(REYE_IDX) // 3) : 3 * (len(REYE_IDX) // 3)]\n","        reye = tf.concat(\n","            [reye_x[..., tf.newaxis], reye_y[..., tf.newaxis], reye_z[..., tf.newaxis]],\n","            axis=-1,\n","        )\n","        leye_x = leye[:, 0 * (len(LEYE_IDX) // 3) : 1 * (len(LEYE_IDX) // 3)]\n","        leye_y = leye[:, 1 * (len(LEYE_IDX) // 3) : 2 * (len(LEYE_IDX) // 3)]\n","        leye_z = leye[:, 2 * (len(LEYE_IDX) // 3) : 3 * (len(LEYE_IDX) // 3)]\n","        leye = tf.concat(\n","            [leye_x[..., tf.newaxis], leye_y[..., tf.newaxis], leye_z[..., tf.newaxis]],\n","            axis=-1,\n","        )\n","    else:\n","        reye_x = reye[:, 0 * (len(REYE_IDX) // 2) : 1 * (len(REYE_IDX) // 2)]\n","        reye_y = reye[:, 1 * (len(REYE_IDX) // 2) : 2 * (len(REYE_IDX) // 2)]\n","        leye_x = leye[:, 0 * (len(LEYE_IDX) // 2) : 1 * (len(LEYE_IDX) // 2)]\n","        leye_y = leye[:, 1 * (len(LEYE_IDX) // 2) : 2 * (len(LEYE_IDX) // 2)]\n","        reye = tf.concat([reye_x[..., tf.newaxis], reye_y[..., tf.newaxis]], axis=-1)\n","        leye = tf.concat([leye_x[..., tf.newaxis], leye_y[..., tf.newaxis]], axis=-1)\n","\n","    face = tf.concat([lip,nose,reye,leye], axis=1)\n","\n","\n","    lpose = tf.gather(x, LPOSE_IDX, axis=1)\n","    rpose = tf.gather(x, RPOSE_IDX, axis=1)\n","    if USE_Z_AXIS:\n","        lpose_x = lpose[:, 0 * (len(LPOSE_IDX) // 3) : 1 * (len(LPOSE_IDX) // 3)]\n","        lpose_y = lpose[:, 1 * (len(LPOSE_IDX) // 3) : 2 * (len(LPOSE_IDX) // 3)]\n","        lpose_z = lpose[:, 2 * (len(LPOSE_IDX) // 3) : 3 * (len(LPOSE_IDX) // 3)]\n","        lpose = tf.concat([lpose_x[..., tf.newaxis], lpose_y[..., tf.newaxis], lpose_z[..., tf.newaxis]], axis=-1)\n","        rpose_x = rpose[:, 0 * (len(RPOSE_IDX) // 3) : 1 * (len(RPOSE_IDX) // 3)]\n","        rpose_y = rpose[:, 1 * (len(RPOSE_IDX) // 3) : 2 * (len(RPOSE_IDX) // 3)]\n","        rpose_z = rpose[:, 2 * (len(RPOSE_IDX) // 3) : 3 * (len(RPOSE_IDX) // 3)]\n","        rpose = tf.concat([rpose_x[..., tf.newaxis], rpose_y[..., tf.newaxis], rpose_z[..., tf.newaxis]], axis=-1)\n","\n","    pose = tf.concat([lpose,rpose], axis=1)\n","\n","    x = tf.concat([face[:,:len(LIP)],lhand,rhand,face[:,len(LIP):],pose], axis=1)\n","    return x\n","\n","\n","\n","def spatial_random_rotation_for_finger(\n","    xyz,\n","    degree=(-10, 10),\n","):\n","    # use first position for rotation center\n","    if USE_Z_AXIS:\n","        xy = xyz[:, :, 0:2]\n","    else:\n","        xy = xyz\n","    center = xy[:,0:1,:]\n","    # center = tf.reduce_mean(xy, axis=[0,1])\n","    if degree is not None:\n","        xy -= center\n","        degree = tf.random.uniform(shape=[], minval=degree[0], maxval=degree[1], dtype=tf.float32)\n","        radian = degree / 180 * np.pi\n","        c = tf.math.cos(radian)\n","        s = tf.math.sin(radian)\n","        rotate_mat = tf.identity(\n","            [\n","                [c, s],\n","                [-s, c],\n","            ]\n","        )\n","        xy = xy @ rotate_mat\n","        xy = xy + center\n","    if USE_Z_AXIS:\n","        return tf.concat([xy, xyz[:, :, 2:3]], axis=-1)\n","    else:\n","        return xy\n","\n","def spatial_random_scale_for_finger(\n","    xyz,\n","    scale=(0.9, 1.1),\n","):\n","    # use first position for rotation center\n","    if USE_Z_AXIS:\n","        xy = xyz[:, :, 0:2]\n","    else:\n","        xy = xyz\n","    center = xy[:,0:1,:]\n","    # center = tf.reduce_mean(xy, axis=[0,1])\n","    if scale is not None:\n","        xy -= center\n","        scale = tf.random.uniform(shape=[], minval=scale[0], maxval=scale[1], dtype=tf.float32)\n","        xy = xy * scale\n","        xy = xy + center\n","    if USE_Z_AXIS:\n","        return tf.concat([xy, xyz[:, :, 2:3]], axis=-1)\n","    else:\n","        return xy\n","\n","def only_rotate(xyz,degree=(-15,15),shear = (-0.10,0.10),scale  = (0.75,1.5),\n","                ):\n","    scale = tf.random.uniform((),*scale)\n","    xyz *= scale\n","    if USE_Z_AXIS:\n","        xy = xyz[:, :, 0:2]\n","    else:\n","        xy = xyz\n","    center = tf_nan_mean(xy, axis=[0,1])\n","    degree = tf.random.uniform((),*degree)\n","    xy -= center\n","\n","    radian = degree/180*np.pi\n","    c = tf.math.cos(radian)\n","    s = tf.math.sin(radian)\n","    rotate_mat = tf.identity([\n","        [c,s],\n","        [-s, c],\n","    ])\n","    xy = xy @ rotate_mat\n","\n","\n","    shear_x = shear_y = tf.random.uniform((),*shear)\n","    if tf.random.uniform(()) < 0.5:\n","        shear_x = 0.\n","    else:\n","        shear_y = 0.\n","    shear_mat = tf.identity([\n","        [1.,shear_x],\n","        [shear_y,1.]\n","    ])\n","    xy = xy @ shear_mat\n","\n","\n","\n","    xy = xy + center\n","    if USE_Z_AXIS:\n","        return tf.concat([xy, xyz[:, :, 2:3]], axis=-1)\n","    else:\n","        return xy\n","\n","def inner_flip(x):\n","    x,y,z = tf.unstack(x, axis=-1)\n","    x = 2*tf_nan_mean(x, axis=[0,1], keepdims=True) -x\n","    new_x = tf.stack([x,y,z], -1)\n","    return new_x\n","\n","def flip_lr(x):\n","    x,y,z = tf.unstack(x, axis=-1)\n","    face = tf.concat([x[:,:len(LIP)], x[:,len(LIP)+len(LHAND)+len(RHAND):-POSE_NUMS]], axis=1)\n","    face_mean = tf_nan_mean(face, axis=[0,1], keepdims=True)\n","    x = 2*face_mean -x\n","    # x = 1 - x\n","    new_x = tf.stack([x,y,z], -1)\n","\n","    face = tf.concat([new_x[:,:len(LIP),:], new_x[:,len(LIP)+len(LHAND)+len(RHAND):-POSE_NUMS,:]], axis=1)\n","    leye = face[:,-len(LEYE):,:]\n","    reye = face[:,-len(REYE)-len(LEYE):-len(LEYE),:]\n","    face = tf.concat([face[:,:-len(LEYE)-len(REYE),:],leye,reye], axis=1)\n","    rhand = new_x[:,len(LIP)+len(LHAND):len(LIP)+len(LHAND)+len(RHAND),:]\n","    lhand = new_x[:,len(LIP):len(LIP)+len(LHAND),:]\n","    pose = new_x[:,-POSE_NUMS:,:]\n","    new_x = tf.concat([face[:,:len(LIP)],rhand, lhand,face[:,len(LIP):],pose], axis=1)\n","\n","    lip = new_x[:,:len(LIP),:]\n","    lip = inner_flip(lip)\n","    nose = new_x[:,len(LIP)+len(LHAND)+len(RHAND):len(LIP)+len(LHAND)+len(RHAND)+len(NOSE),:]\n","    nose = inner_flip(nose)\n","    pose = new_x[:,-POSE_NUMS:,:]\n","    rpose = pose[:,POSE_NUMS//2:,:]\n","    lpose = pose[:,:POSE_NUMS//2,:]\n","    pose = tf.concat([rpose,lpose], axis=1)\n","    new_x = tf.concat([lip, new_x[:,len(LIP):len(LIP)+len(LHAND),:], new_x[:,len(LIP)+len(LHAND):len(LIP)+len(LHAND)+len(RHAND),:], nose,new_x[:,len(LIP)+len(LHAND)+len(RHAND)+len(NOSE):-POSE_NUMS,:],pose], axis=1)\n","    return new_x\n","\n","\n","\n","def interp1d_(x, target_len, method=\"random\"):\n","    target_len = tf.maximum(1, target_len)\n","    if method == \"random\":\n","        if tf.random.uniform(()) < 0.33:\n","            x = tf.image.resize(x, (target_len, tf.shape(x)[1]), \"bilinear\")\n","        else:\n","            if tf.random.uniform(()) < 0.5:\n","                x = tf.image.resize(x, (target_len, tf.shape(x)[1]), \"bicubic\")\n","            else:\n","                x = tf.image.resize(x, (target_len, tf.shape(x)[1]), \"nearest\")\n","    else:\n","        x = tf.image.resize(x, (target_len, tf.shape(x)[1]), method)\n","    return x\n","\n","\n","def resample(x, rate):\n","    # re-resample\n","    rate = tf.random.uniform((),rate[0], rate[1])\n","\n","    length = tf.shape(x)[0]\n","\n","    new_size = tf.cast(rate * tf.cast(length, tf.float32), tf.int32)\n","    return interp1d_(x, new_size)\n","\n","def uniform_resample(x):\n","    l = 10/tf.cast(tf.shape(x)[0], tf.float32)\n","    r = FRAME_LEN/tf.cast(tf.shape(x)[0], tf.float32)\n","    return resample(x, (l,r))\n","\n","def resample_sub(x, rate=(0.5, 1.5)):\n","\n","    if tf.shape(x)[0] < 2:\n","        x = resample(x, (1.0,5.0))\n","        return x\n","    if tf.random.uniform(()) < 0.5:\n","        start = tf.random.uniform(shape=[], minval=0, maxval=tf.shape(x)[0]-1, dtype=tf.int32)\n","        end = tf.random.uniform(shape=[], minval=start+1, maxval=tf.shape(x)[0], dtype=tf.int32)\n","        if start > end:\n","            start, end = end, start\n","        x = tf.concat([x[:start], resample(x[start:end], rate), x[end:]], axis=0)\n","    else:\n","        x = resample(x, rate)\n","    return x\n","\n","\n","def mask_along_axis(tensor, param_min, param_max, axis, mask_value=float('nan')):\n","    tensor_shape = tf.shape(tensor)\n","    dim_size = tensor_shape[axis]\n","    min_mask_size = tf.cast(param_min * tf.cast(dim_size, tf.float32),tf.int32)\n","    max_mask_size = tf.cast(param_max * tf.cast(dim_size, tf.float32), tf.int32)\n","    mask_size = tf.cond(\n","        tf.equal(min_mask_size, max_mask_size),\n","        lambda: min_mask_size,\n","        lambda: tf.random.uniform((), min_mask_size, max_mask_size+1, dtype=tf.int32)\n","    )\n","    mask_start = tf.random.uniform([], 0, dim_size - mask_size, tf.int32)\n","    indices = tf.cast(tf.range(start=0, limit=dim_size, delta=1),tf.int32)\n","    mask = tf.logical_or(indices < mask_start , indices >= (mask_start + mask_size))\n","    if axis==1:\n","        mask = tf.reshape(tf.cast(mask, tf.float32),(1,dim_size,1))\n","    else:\n","        mask = tf.reshape(tf.cast(mask, tf.float32),(dim_size,1,1))\n","    masked_tensor = tf.where(mask==0,mask_value,tensor)\n","    # masked_tensor = mask * tensor\n","    return masked_tensor\n","\n","def discrete_mask(tensor, param_min, param_max, axis,mask_value=float('nan')):\n","    tensor_shape = tf.shape(tensor)\n","    dim_size = tensor_shape[axis]\n","\n","    min_mask_size = tf.cast(param_min * tf.cast(dim_size, tf.float32), tf.int32)\n","    max_mask_size = tf.cast(param_max * tf.cast(dim_size, tf.float32), tf.int32)\n","\n","    mask_size = tf.cond(\n","        tf.equal(min_mask_size, max_mask_size),\n","        lambda: min_mask_size,\n","        lambda: tf.random.uniform((), min_mask_size, max_mask_size + 1, dtype=tf.int32)\n","    )\n","\n","    mask_indices = tf.random.shuffle(tf.range(dim_size))[:mask_size]\n","\n","    mask = tf.scatter_nd(\n","        tf.expand_dims(mask_indices, 1),\n","        tf.ones(mask_size, dtype=tf.float32),\n","        [dim_size]\n","    )\n","\n","    if axis == 1:\n","        mask = tf.reshape(mask, (1, dim_size, 1))\n","    else:\n","        mask = tf.reshape(mask, (dim_size, 1, 1))\n","\n","    masked_tensor = tf.where(mask==0,mask_value,tensor)\n","    return masked_tensor\n","\n","\n","\n","fingers = [[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16],[17,18,19,20]]\n","\n","\n","def random_rotate_fingers(x):\n","    # x shape: (seq_len, 21, 2)\n","    finger1 = tf.gather(x, fingers[0], axis=1)\n","    finger2 = tf.gather(x, fingers[1], axis=1)\n","    finger3 = tf.gather(x, fingers[2], axis=1)\n","    finger4 = tf.gather(x, fingers[3], axis=1)\n","    finger5 = tf.gather(x, fingers[4], axis=1)\n","    finger1 = spatial_random_rotation_for_finger(finger1)\n","    finger2 = spatial_random_rotation_for_finger(finger2)\n","    finger3 = spatial_random_rotation_for_finger(finger3)\n","    finger4 = spatial_random_rotation_for_finger(finger4)\n","    finger5 = spatial_random_rotation_for_finger(finger5)\n","    finger1 = spatial_random_scale_for_finger(finger1)\n","    finger2 = spatial_random_scale_for_finger(finger2)\n","    finger3 = spatial_random_scale_for_finger(finger3)\n","    finger4 = spatial_random_scale_for_finger(finger4)\n","    finger5 = spatial_random_scale_for_finger(finger5)\n","    hand_root = tf.expand_dims(x[:,0,:], axis=1)\n","    x = tf.concat([hand_root, finger1, finger2, finger3, finger4, finger5], axis=1)\n","    return x\n","\n","\n","def spatial_mask(x, size=(0.1,0.4), mask_value=float('nan')):\n","    x_min = tf.math.reduce_min(x[~tf.math.is_nan(x[...,0])])\n","    x_max = tf.math.reduce_max(x[~tf.math.is_nan(x[...,0])])\n","    y_min = tf.math.reduce_min(x[~tf.math.is_nan(x[...,1])])\n","    y_max = tf.math.reduce_max(x[~tf.math.is_nan(x[...,1])])\n","    mask_offset_x = tf.random.uniform((1,), x_min, x_max)\n","    mask_offset_y = tf.random.uniform((1,), y_min, y_max)\n","    mask_size_x = tf.random.uniform((1,), *size) * (x_max - x_min)\n","    mask_size_y = tf.random.uniform((1,), *size) * (y_max - y_min)\n","    mask_x = (mask_offset_x<x[...,0]) & (x[...,0] < mask_offset_x + mask_size_x)\n","    mask_y = (mask_offset_y<x[...,1]) & (x[...,1] < mask_offset_y + mask_size_y)\n","    mask = mask_x & mask_y\n","    x = tf.where(mask[...,None], mask_value, x)\n","    return x\n","\n","\n","def temporal_mask(x, size=(0.1,0.3), mask_value=float('nan')):\n","    l = tf.shape(x)[0]\n","    mask_size = tf.random.uniform((), *size)\n","    mask_size = tf.cast(tf.cast(l, tf.float32) * mask_size, tf.int32)\n","    mask_offset = tf.random.uniform((), 0, tf.clip_by_value(l-mask_size,1,l), dtype=tf.int32)\n","    x = tf.tensor_scatter_nd_update(x,tf.range(mask_offset, mask_offset+mask_size)[...,None],tf.fill([mask_size,KPT_NUM//DATA_DIM,DATA_DIM],mask_value))\n","    return x\n","\n","\n","def random_shift(x, shift_range=0.1):\n","    # x shape TxKxdim\n","    shift_var = tf.random.uniform((1,1,3),-shift_range,shift_range)\n","    shift_var = tf.tile(shift_var, (tf.shape(x)[0], tf.shape(x)[1], 1))\n","    x = x + shift_var\n","    return x\n","\n","def rotate_partial(x,partial=False):\n","    def apply(x):\n","        face = tf.concat([x[:,:len(LIP),:], x[:,len(LIP)+len(LHAND)+len(RHAND):-POSE_NUMS,:]], axis=1)\n","        face = only_rotate(face,degree=(-30,30))\n","        x = tf.concat([face[:,:len(LIP),:],x[:,len(LIP):len(LIP)+len(LHAND)+len(RHAND),:], face[:,len(LIP):,:],x[:,-POSE_NUMS:,:]], axis=1)\n","        lhand = x[:,len(LIP):len(LIP)+len(LHAND),:]\n","        lhand = only_rotate(lhand,degree=(-30,30))\n","        x = tf.concat([x[:,:len(LIP),:], lhand, x[:,len(LIP)+len(LHAND):,:]], axis=1)\n","        rhand = x[:,len(LIP)+len(LHAND):len(LIP)+len(LHAND)+len(RHAND),:]\n","        rhand = only_rotate(rhand,degree=(-30,30))\n","        x = tf.concat([x[:,:len(LIP)+len(LHAND),:], rhand, x[:,len(LIP)+len(LHAND)+len(RHAND):,:]], axis=1)\n","        pose = x[:,-POSE_NUMS:,:]\n","        pose = only_rotate(pose,degree=(-30,30))\n","        x = tf.concat([x[:,:-POSE_NUMS,:], pose], axis=1)\n","        return x\n","    if tf.shape(x)[0] > 10 and partial:\n","        start = tf.random.uniform(shape=[], minval=0, maxval=tf.shape(x)[0]-1, dtype=tf.int32)\n","        end = tf.random.uniform(shape=[], minval=start+1, maxval=tf.shape(x)[0], dtype=tf.int32)\n","        x = tf.concat([x[:start], apply(x[start:end]), x[end:]], axis=0)\n","    else:\n","        return apply(x)\n","    return x\n","\n","def rotate_finger_partial(x,partial=False):\n","    def apply(x):\n","        lhand = x[:,len(LIP):len(LIP)+len(LHAND),:]\n","        lhand = random_rotate_fingers(lhand)\n","        rhand = x[:,len(LIP)+len(LHAND):len(LIP)+len(LHAND)+len(RHAND),:]\n","        rhand = random_rotate_fingers(rhand)\n","        x = tf.concat([x[:,:len(LIP),:], lhand, rhand, x[:,len(LIP)+len(LHAND)+len(RHAND):,:]], axis=1)\n","        return x\n","    if tf.shape(x)[0] > 10 and partial:\n","        start = tf.random.uniform(shape=[], minval=0, maxval=tf.shape(x)[0]-1, dtype=tf.int32)\n","        end = tf.random.uniform(shape=[], minval=start+1, maxval=tf.shape(x)[0], dtype=tf.int32)\n","        x = tf.concat([x[:start], apply(x[start:end]), x[end:]], axis=0)\n","    else:\n","        return apply(x)\n","    return x\n","\n","def shift_partial(x,partial=False):\n","    def apply(x):\n","        face = tf.concat([x[:,:len(LIP),:], x[:,len(LIP)+len(LHAND)+len(RHAND):-POSE_NUMS,:]], axis=1)\n","        face = random_shift(face)\n","        lhand = x[:,len(LIP):len(LIP)+len(LHAND),:]\n","        lhand = random_shift(lhand)\n","        rhand = x[:,len(LIP)+len(LHAND):len(LIP)+len(LHAND)+len(RHAND),:]\n","        rhand = random_shift(rhand)\n","        pose = x[:,-POSE_NUMS:,:]\n","        pose = random_shift(pose)\n","        x = tf.concat([face[:,:len(LIP),:], lhand, rhand, face[:,len(LIP):,:],pose], axis=1)\n","\n","        return x\n","    if tf.shape(x)[0] > 10 and partial:\n","        start = tf.random.uniform(shape=[], minval=0, maxval=tf.shape(x)[0]-1, dtype=tf.int32)\n","        end = tf.random.uniform(shape=[], minval=start+1, maxval=tf.shape(x)[0], dtype=tf.int32)\n","        x = tf.concat([x[:start], apply(x[start:end]), x[end:]], axis=0)\n","    else:\n","        return apply(x)\n","    return x\n","\n","\n","def combined_mask_along_axis(tensor, s1, s2, mask_value=float('nan')):\n","    def get_mask(tensor, param_min, param_max, axis, mask_value=float('nan')):\n","        tensor_shape = tf.shape(tensor)\n","        dim_size = tensor_shape[axis]\n","        min_mask_size = tf.cast(param_min * tf.cast(dim_size, tf.float32),tf.int32)\n","        max_mask_size = tf.cast(param_max * tf.cast(dim_size, tf.float32), tf.int32)\n","        mask_size = tf.cond(\n","            tf.equal(min_mask_size, max_mask_size),\n","            lambda: min_mask_size,\n","            lambda: tf.random.uniform((), min_mask_size, max_mask_size+1, dtype=tf.int32)\n","        )\n","        mask_start = tf.random.uniform([], 0, dim_size - mask_size, tf.int32)\n","        indices = tf.cast(tf.range(start=0, limit=dim_size, delta=1),tf.int32)\n","        mask = tf.logical_or(indices < mask_start , indices >= (mask_start + mask_size))\n","        if axis==1:\n","            mask = tf.reshape(mask,(1,dim_size,1))\n","        else:\n","            mask = tf.reshape(mask,(dim_size,1,1))\n","        return mask\n","    t_mask = get_mask(tensor, s1[0], s1[1], 1, mask_value)\n","    f_mask = get_mask(tensor, s2[0], s2[1], 0, mask_value)\n","    mask = tf.logical_or(t_mask, f_mask)\n","    mask = tf.cast(mask, tf.float32)\n","    masked_tensor = tf.where(mask==0,mask_value,tensor)\n","    return masked_tensor\n","\n","def augment_fn(x):\n","    if tf.random.uniform(()) > 0.33:\n","        x = resample_sub(x)\n","        # if tf.random.uniform(()) > 0.5:\n","        #     x = resample_sub(x) # temporal\n","        # else:\n","        #     x = uniform_resample(x) # temporal\n","    if tf.random.uniform(()) > 0.5:\n","        # filp face\n","        x = flip_lr(x)\n","\n","    if tf.random.uniform(()) < 0.5:\n","        if tf.random.uniform(()) < 0.5:\n","            x = rotate_partial(x,partial=True)\n","        else:\n","            x = rotate_partial(x,partial=False)\n","\n","    if tf.random.uniform(()) < 0.5:\n","        if tf.random.uniform(()) < 0.5:\n","            x = rotate_finger_partial(x,partial=True)\n","        else:\n","            x = rotate_finger_partial(x,partial=False)\n","\n","    # # ramdom shift\n","    if tf.random.uniform(()) > 0.5:\n","        if tf.random.uniform(()) < 0.5:\n","            x = shift_partial(x,partial=True)\n","        else:\n","            x = shift_partial(x,partial=False)\n","\n","    if tf.random.uniform(()) < 0.5:\n","        # T = tf.minimum(tf.cast(tf.shape(x)[0],tf.float32),200.0)\n","        # factor = T/200 * 0.3\n","        if tf.random.uniform(()) > 0.33:\n","            x = mask_along_axis(x,0.0,0.4,0) # originally 0.2~0.4, can change to 0.1~0.3 for faster convergence\n","        elif tf.random.uniform(()) > 0.5:\n","            x = temporal_mask(x,size=(0.0,0.4))\n","        else:\n","            x = discrete_mask(x,0.0,0.4,0)\n","\n","    if tf.random.uniform(()) < 0.5:\n","        # spatial masking\n","        if tf.random.uniform(()) > 0.33:\n","            x = mask_along_axis(x,0.0,0.4,1) # can be 0.2,0.4\n","        elif tf.random.uniform(()) > 0.5:\n","            x = spatial_mask(x,size=(0.0,0.4))\n","        else:\n","            x = discrete_mask(x,0.0,0.4,1)\n","\n","\n","    # x = tf.where(tf.math.is_nan(x),NAN_FILL_VALUE,x)\n","    return x\n","\n","\n","KPT_NUM = (len(LIP) + len(LHAND) + len(RHAND) + len(NOSE) + len(LEYE) + len(REYE) + len(LPOSE) + len(RPOSE)) * DATA_DIM\n","\n","\n","def preprocess1(x):\n","    x = split_data(x)\n","    # x = tf.where(tf.math.is_nan(x), NAN_FILL_VALUE, x)\n","    return x\n","\n","\n","def preprocess2(x):\n","    # x = global_norm(x)\n","    face = tf.concat([x[:,:len(LIP),:], x[:,len(LIP)+len(LHAND)+len(RHAND):-POSE_NUMS,:]], axis=1)\n","    pose = x[:, -POSE_NUMS:, :]\n","    lhand = x[:,len(LIP):len(LIP)+len(LHAND),:]\n","    rhand = x[:,len(LIP)+len(LHAND):len(LIP)+len(LHAND)+len(RHAND),:]\n","    face = self_norm(face)\n","    lhand = self_norm(lhand)\n","    rhand = self_norm(rhand)\n","    pose = self_norm(pose)\n","    x = tf.concat([face, lhand, rhand, pose], axis=1)\n","    # x = tf.concat([face, lhand, rhand], axis=1)\n","    # x = tf.concat([face[:,:len(LIP),:], x[:,len(LIP):len(LIP)+len(LHAND)+len(RHAND),:],face[:,len(LIP):,:],x[:,-POSE_NUMS:,:]], axis=1)\n","    # x = x[...,:2]\n","\n","\n","    x = resize_pad(x)\n","\n","\n","    dx = x[1:,:,:] - x[:-1,:,:]\n","    dx = tf.concat([tf.zeros((1, KPT_NUM//DATA_DIM, tf.shape(x)[-1])), dx], axis=0) # Tx21x2\n","    if tf.shape(x)[0] > 1:\n","        dx2 = x[2:,:,:] - x[:-2,:,:]\n","        dx2 = tf.concat([tf.zeros((2, KPT_NUM//DATA_DIM, tf.shape(x)[-1])), dx2], axis=0) # Tx21x2\n","    else:\n","        dx2 = tf.zeros_like(dx)\n","    x = tf.concat([x, dx,dx2], axis=-1)\n","\n","    x = tf.reshape(x, (FRAME_LEN, INPUT_DIM))\n","    x = tf.where(tf.math.is_nan(x), NAN_FILL_VALUE, x)\n","\n","    return x\n","\n","def total_process(x, phrase, augment=False):\n","\n","    x = preprocess1(x)\n","    if augment:\n","        x = augment_fn(x)\n","    x = preprocess2(x)\n","    return x,phrase\n","\n","INPUT_DIM = KPT_NUM * 3 # coordinate + velocity\n","TEMPORAL_DIM = FRAME_LEN\n","\n","def preprocess_(x):\n","    x = preprocess1(x)\n","    x = preprocess2(x)\n","    return x\n","\n","def preprocess_fn(x, phrase, augment=False):\n","    batch = total_process(x, phrase,augment)\n","    return batch\n","\n","\n","print(TEMPORAL_DIM,INPUT_DIM)"]},{"cell_type":"markdown","metadata":{},"source":["# Train/VAL files as Tensors (Include z)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["table = tf.lookup.StaticHashTable(\n","    initializer=tf.lookup.KeyValueTensorInitializer(\n","        keys=list(CHAR2ORD.keys()),\n","        values=list(CHAR2ORD.values()),\n","    ),\n","    default_value=tf.constant(-1),\n","    name=\"class_weight\",\n",")\n","\n","\n","def decode_fn(record_bytes):\n","    schema = {COL: tf.io.VarLenFeature(dtype=tf.float32) for COL in SEL_COLS}\n","    schema[\"phrase\"] = tf.io.FixedLenFeature([], dtype=tf.string)\n","    features = tf.io.parse_single_example(record_bytes, schema)\n","\n","    phrase = features[\"phrase\"]\n","    landmarks = [tf.sparse.to_dense(features[COL]) for COL in SEL_COLS]\n","    landmarks = tf.transpose(landmarks)\n","    phrase = tf.strings.bytes_split(phrase)\n","\n","    phrase = table.lookup(phrase)\n","    phrase = tf.pad(\n","        phrase,\n","        paddings=[[0, MAX_PHRASE_LENGTH - tf.shape(phrase)[0]]],\n","        constant_values=PAD_TOKEN,\n","    )\n","    return landmarks, phrase\n","\n","\n","val_file_ids = [234418913]\n","if VAL:\n","    val_tffiles = train[train.file_id.isin(val_file_ids)].file_id.map(lambda x: f\"/kaggle/input/aslfr-preprocess-dataset/tfds-v2/{x}.tfrecord\").unique()\n","    train_tffiles = train[~train.file_id.isin(val_file_ids)].file_id.map(lambda x: f\"/kaggle/input/aslfr-preprocess-dataset/tfds-v2/{x}.tfrecord\").unique()\n","else:\n","    train_tffiles = train.file_id.map(lambda x: f\"/kaggle/input/aslfr-preprocess-dataset/tfds-v2/{x}.tfrecord\").unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# val_len = int(len(tffiles) * 0.05)\n","val_size = BATCH_SIZE\n","if VAL:\n","    print(' Train Val Split')\n","    train_dataset = tf.data.TFRecordDataset(train_tffiles,num_parallel_reads=tf.data.AUTOTUNE\n","                                            ).map(decode_fn,num_parallel_calls=tf.data.AUTOTUNE).map(lambda x, phrase: preprocess_fn(x, phrase,augment=True),num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE).repeat().prefetch(buffer_size=tf.data.AUTOTUNE)\n","\n","    val_dataset = tf.data.TFRecordDataset(val_tffiles,num_parallel_reads=tf.data.AUTOTUNE\n","                                          ).map(decode_fn,num_parallel_calls=tf.data.AUTOTUNE).map(preprocess_fn,tf.data.AUTOTUNE).batch(BATCH_SIZE).repeat().prefetch(buffer_size=tf.data.AUTOTUNE)\n","    aux_dataset = tf.data.TFRecordDataset(train_tffiles[:1],num_parallel_reads=tf.data.AUTOTUNE\n","                                          ).map(decode_fn,num_parallel_calls=tf.data.AUTOTUNE).map(preprocess_fn,tf.data.AUTOTUNE).batch(BATCH_SIZE).repeat().prefetch(buffer_size=tf.data.AUTOTUNE)\n","    print('train size', 64211+1997)\n","    print('val size', 1000)\n","    print('train datafiles',train_tffiles)\n","    print('val datafiles',val_tffiles)\n","else:\n","    print('Full Train')\n","    train_dataset = tf.data.TFRecordDataset(train_tffiles, num_parallel_reads=tf.data.AUTOTUNE\n","                                             ).map(decode_fn,tf.data.AUTOTUNE).map(lambda x, phrase: preprocess_fn(x, phrase,augment=True),num_parallel_calls=tf.data.AUTOTUNE).batch(BATCH_SIZE).repeat().prefetch(buffer_size=tf.data.AUTOTUNE)\n","\n","    print('train size', 64211+2997)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Training Steps Per Epoch\n","TRAIN_STEPS_PER_EPOCH = math.ceil(64211+1997 / BATCH_SIZE)\n","print(f'TRAIN_STEPS_PER_EPOCH: {TRAIN_STEPS_PER_EPOCH}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if VAL:\n","    N_VAL_STEPS_PER_EPOCH = math.ceil(1000 / BATCH_SIZE)\n","    print(f'N_VAL_STEPS_PER_EPOCH: {N_VAL_STEPS_PER_EPOCH}')"]},{"cell_type":"markdown","metadata":{},"source":["# Learning Rate logger"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class LearningRateLogger(tf.keras.callbacks.Callback):\n","    def on_epoch_end(self, epoch, logs=None):\n","        lr = self.model.optimizer.lr\n","        # If the learning rate is a decayed value, compute its value\n","        if isinstance(lr, tf.keras.optimizers.schedules.LearningRateSchedule):\n","            lr = lr(self.model.optimizer.iterations)\n","        print(f\"\\nLearning rate at end of epoch {epoch}: {lr.numpy()}\\n\")\n","\n","# Instantiate the callback\n","lr_logger = LearningRateLogger()"]},{"cell_type":"markdown","metadata":{},"source":["# Learning Rate Scheduler for tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class OneCycleLR(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    '''\n","    Unified single-cycle learning rate scheduler for tensorflow.\n","    2022 Hoyeol Sohn <hoeyol0730@gmail.com>\n","    '''\n","    def __init__(self,\n","                lr=1e-3,\n","                epochs=10,\n","                steps_per_epoch=100,\n","                steps_per_update=1,\n","                resume_epoch=0,\n","                decay_epochs=10,\n","                sustain_epochs=0,\n","                warmup_epochs=10,\n","                lr_start=0,\n","                lr_min=0,\n","                warmup_type='linear',\n","                decay_type='cosine',\n","                finetune_steps=0,\n","                finetune_lr=1e-5,\n","                **kwargs):\n","\n","        super().__init__(**kwargs)\n","        self.lr = float(lr)\n","        self.epochs = float(epochs)\n","        self.steps_per_update = float(steps_per_update)\n","        self.resume_epoch = float(resume_epoch)\n","        self.steps_per_epoch = float(steps_per_epoch)\n","        self.decay_epochs = float(decay_epochs)\n","        self.sustain_epochs = float(sustain_epochs)\n","        self.warmup_epochs = float(warmup_epochs)\n","        self.lr_start = float(lr_start)\n","        self.lr_min = float(lr_min)\n","        self.decay_type = decay_type\n","        self.warmup_type = warmup_type\n","        self.finetune_steps = finetune_steps\n","        self.finetune_lr = float(finetune_lr)\n","\n","    def __call__(self, step):\n","        step = tf.cast(step, tf.float32)\n","        total_steps = self.epochs * self.steps_per_epoch\n","        warmup_steps = self.warmup_epochs * self.steps_per_epoch\n","        sustain_steps = self.sustain_epochs * self.steps_per_epoch\n","        decay_steps = self.decay_epochs * self.steps_per_epoch\n","\n","        if self.resume_epoch > 0:\n","            step = step + self.resume_epoch * self.steps_per_epoch\n","\n","        step = tf.cond(step > decay_steps, lambda :decay_steps, lambda :step)\n","        step = tf.math.truediv(step, self.steps_per_update) * self.steps_per_update\n","\n","        warmup_cond = step < warmup_steps\n","        decay_cond = step >= (warmup_steps + sustain_steps)\n","        finetune_cond = step >= (total_steps - self.finetune_steps)\n","\n","        lr = tf.cond(warmup_cond, lambda: tf.math.divide_no_nan(self.lr-self.lr_start , warmup_steps) * step + self.lr_start, lambda: self.lr)\n","\n","        lr = tf.cond(decay_cond, lambda: 0.5 * (self.lr - self.lr_min) * (1 + tf.cos(3.14159265359 * (step - warmup_steps - sustain_steps) / (decay_steps - warmup_steps - sustain_steps))) + self.lr_min, lambda:lr)\n","\n","        lr = tf.cond(finetune_cond, lambda: tf.constant(self.finetune_lr,tf.float32), lambda:lr)\n","        return lr"]},{"cell_type":"markdown","metadata":{},"source":["# Landmark Embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# copy from https://www.kaggle.com/code/markwijkhuizen/aslfr-transformer-training-inference#Landmark-Embedding\n","class LandmarkEmbedding(tf.keras.layers.Layer):\n","    def __init__(self, units, name='emb'):\n","        super(LandmarkEmbedding, self).__init__(name=f'{name}_embedding')\n","        self.supports_masking = True\n","        \n","        # Embedding\n","        self.dense = tf.keras.Sequential([\n","            tf.keras.layers.Dense((units+INPUT_DIM)//2,activation='swish'),\n","            tf.keras.layers.Dense(units),\n","            tf.keras.layers.BatchNormalization(),\n","        ], name=f'{name}_dense')\n","\n","    def call(self, x):\n","        return self.dense(x)"]},{"cell_type":"markdown","metadata":{},"source":["# Conformer"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# based on: https://stackoverflow.com/questions/67342988/verifying-the-implementation-of-multihead-attention-in-transformer\n","# replaced softmax with softmax layer to support masked softmax\n","class LateDropout(tf.keras.layers.Layer):\n","    def __init__(self, rate, noise_shape=None, start_step=0, **kwargs):\n","        super().__init__(**kwargs)\n","        self.supports_masking = True\n","        self.rate = rate\n","        self.start_step = start_step\n","        self.dropout = tf.keras.layers.Dropout(rate, noise_shape=noise_shape)\n","\n","    def build(self, input_shape):\n","        super().build(input_shape)\n","        agg = tf.VariableAggregation.ONLY_FIRST_REPLICA\n","        self._train_counter = tf.Variable(0, dtype=\"int64\", aggregation=agg, trainable=False)\n","\n","    def call(self, inputs, training=False):\n","        x = tf.cond(self._train_counter < self.start_step, lambda:inputs, lambda:self.dropout(inputs, training=training))\n","        if training:\n","            self._train_counter.assign_add(1)\n","        return x\n","\n","class CausalDWConv1D(tf.keras.layers.Layer):\n","    def __init__(self,\n","        kernel_size=17,\n","        dilation_rate=1,\n","        use_bias=True,\n","        depthwise_initializer='glorot_uniform',\n","        name='', **kwargs):\n","        super().__init__(name=name,**kwargs)\n","        self.causal_pad = tf.keras.layers.ZeroPadding1D((dilation_rate*(kernel_size-1),0))\n","        self.dw_conv = tf.keras.layers.DepthwiseConv1D(\n","                            kernel_size,\n","                            strides=1,\n","                            dilation_rate=dilation_rate,\n","                            padding='valid',\n","                            use_bias=use_bias,\n","                            depthwise_initializer=depthwise_initializer,)\n","        self.supports_masking = True\n","\n","    def call(self, inputs):\n","        x = self.causal_pad(inputs)\n","        x = self.dw_conv(x)\n","        return x\n","\n","class GLU(tf.keras.layers.Layer):\n","    def __init__(self, dim, **kwargs):\n","        super(GLU, self).__init__(**kwargs)\n","        self.dim = dim\n","        self.supports_masking = True\n","\n","    def call(self, inputs):\n","        out, gate = tf.split(inputs, 2, axis=self.dim)\n","        return out * tf.sigmoid(gate)\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return mask\n","\n","\n","\n","class CausalConv1D(tf.keras.layers.Layer):\n","    def __init__(self,\n","        hid_dim,\n","        kernel_size=17,\n","        dilation_rate=1,\n","        groups = 1,\n","        name='', **kwargs):\n","        super().__init__()\n","        self.causal_pad = tf.keras.layers.ZeroPadding1D((dilation_rate*(kernel_size-1),0))\n","        self.dw_conv = tf.keras.layers.Conv1D(\n","                            hid_dim,\n","                            kernel_size,\n","                            strides=1,\n","                            dilation_rate=dilation_rate,\n","                            padding='valid',groups=groups)\n","        self.supports_masking = True\n","\n","    def call(self, inputs):\n","        x = self.causal_pad(inputs)\n","        x = self.dw_conv(x)\n","        return x\n","\n","\n","class ConformerConvModule(tf.keras.layers.Layer):\n","    def __init__(self, dim, kernel_size, dropout=DROP_OUT,expansion_factor=2):\n","        super().__init__()\n","        hid_dim = dim * expansion_factor\n","        # self.postNorm = tf.keras.layers.BatchNormalization\n","        self.preNorm = tf.keras.layers.BatchNormalization() #tf.keras.layers.BatchNormalization\n","\n","        if not MASKING:\n","            self.pointwise_conv1 = tf.keras.layers.Conv1D(hid_dim, 1, padding='same')\n","        else:\n","            self.pointwise_conv1 = CausalConv1D(hid_dim, 1 )\n","\n","        self.glu = GLU(-1)\n","\n","        if not MASKING:\n","            self.depthwise_conv = tf.keras.layers.Conv1D(dim, kernel_size, padding='same', groups=dim)\n","        else:\n","            self.depthwise_conv = CausalConv1D(dim, kernel_size, groups=dim)\n","\n","        self.batchnorm = tf.keras.layers.BatchNormalization()\n","        self.activation = tf.keras.layers.Activation('swish')\n","\n","        if not MASKING:\n","            self.pointwise_conv2 = tf.keras.layers.Conv1D(dim, 1, padding='same')\n","        else:\n","            self.pointwise_conv2 = CausalConv1D(dim, 1)\n","\n","        self.add = tf.keras.layers.Add()\n","        self.dropout = tf.keras.layers.Dropout(dropout)\n","        self.supports_masking = True\n","        self.dropout2 = tf.keras.layers.Dropout(dropout) # DW dropout\n","        self.dropout1 = tf.keras.layers.Dropout(dropout)\n","\n","    def call(self, x):\n","        identity = x\n","        x = self.preNorm(x)\n","        x = self.pointwise_conv1(x)\n","        x = self.glu(x)\n","        x = self.dropout1(x)\n","\n","        x = self.depthwise_conv(x)\n","        x = self.batchnorm(x)\n","        x = self.activation(x)\n","        x = self.dropout2(x)\n","\n","        x = self.pointwise_conv2(x)\n","        x = self.dropout(x)\n","        x = self.add([x,identity])\n","        return x\n","\n","\n","class MultiHeadSelfAttention(tf.keras.layers.Layer):\n","    def __init__(self, dim=256, num_heads=4, dropout=DROP_OUT, **kwargs):\n","        super().__init__(**kwargs)\n","        self.dim = dim\n","        self.scale = self.dim ** -0.5\n","        self.num_heads = num_heads\n","        self.qkv = tf.keras.layers.Dense(3 * dim)\n","        self.drop1 = tf.keras.layers.Dropout(dropout)\n","        self.proj = tf.keras.layers.Dense(dim)\n","        self.rel_embedding = self.add_weight(\"rel_embedding\", shape=[FRAME_LEN * 2 - 1, dim // num_heads])\n","        self.supports_masking = True\n","\n","    def call(self, inputs, mask=None):\n","        qkv = self.qkv(inputs)\n","        qkv = tf.keras.layers.Permute((2, 1, 3))(\n","            tf.keras.layers.Reshape(\n","                (-1, self.num_heads, self.dim * 3 // self.num_heads)\n","            )(qkv)\n","        )\n","        q, k, v = tf.split(qkv, [self.dim // self.num_heads] * 3, axis=-1)\n","\n","        # seq_len = tf.shape(q)[-2]\n","        seq_len = FRAME_LEN\n","        rel_indices = tf.range(seq_len)[:, None] - tf.range(seq_len)[None, :] + seq_len - 1\n","        rel_indices = rel_indices[:tf.shape(q)[-2], :tf.shape(q)[-2]]\n","        rel_k = tf.nn.embedding_lookup(self.rel_embedding, rel_indices)\n","        rel_logits = tf.einsum('bhid,ijd->bhij', q, rel_k)\n","\n","        attn = (tf.matmul(q, k, transpose_b=True) + rel_logits) * self.scale\n","\n","        if mask is not None:\n","            mask1 = tf.cast(mask[:, None, None,:],tf.float32)\n","            mask2 = tf.cast(mask[:, None, : , None],tf.float32)\n","            mask = mask2 @ mask1\n","            attn = attn + ((1-mask) * -1e9)\n","\n","        attn = tf.keras.layers.Softmax(axis=-1)(attn)\n","        attn = self.drop1(attn)\n","\n","        x = attn @ v\n","        x = tf.keras.layers.Reshape((-1, self.dim))(\n","            tf.keras.layers.Permute((2, 1, 3))(x)\n","        )\n","        x = self.proj(x)\n","        return x\n","\n","\n","def ConformerBlock(dim=256, num_heads=8, expand=2, attn_dropout=DROP_OUT, drop_rate=DROP_OUT, activation='swish',ksize=11):\n","    def apply(inputs):\n","        x = inputs\n","        # mlp1\n","        x = tf.keras.layers.BatchNormalization()(x) #tf.keras.layers.BatchNormalization(x)\n","        x = tf.keras.layers.Dense(dim*expand, activation=activation)(x)\n","        x = tf.keras.layers.Dropout(drop_rate)(x)\n","        x = tf.keras.layers.Dense(dim, kernel_initializer=\"he_normal\")(x)\n","        x = tf.keras.layers.Dropout(drop_rate)(x)\n","        mlp1_x = tf.keras.layers.Add()([inputs, 0.5*x])\n","        # attn\n","        x = tf.keras.layers.BatchNormalization()(mlp1_x) #tf.keras.layers.BatchNormalization(mlp1_x)\n","        x = MultiHeadSelfAttention(dim=dim,num_heads=num_heads,dropout=attn_dropout)(x)\n","        x = tf.keras.layers.Dropout(drop_rate)(x)\n","        attn_out = tf.keras.layers.Add()([mlp1_x, x])\n","        # attn_out = tf.keras.layers.BatchNormalization(x)\n","        # conv\n","        conv_out = ConformerConvModule(dim,ksize)(attn_out)\n","        # mlp2\n","        x = tf.keras.layers.BatchNormalization()(conv_out) #tf.keras.layers.BatchNormalization(conv_out)\n","        x = tf.keras.layers.Dense(dim*expand, activation=activation)(x)\n","        x = tf.keras.layers.Dropout(drop_rate)(x)\n","        x = tf.keras.layers.Dense(dim, kernel_initializer=\"he_normal\")(x)\n","        x = tf.keras.layers.Dropout(drop_rate)(x)\n","        x = tf.keras.layers.Add()([conv_out, 0.5*x])\n","        # x = tf.keras.layers.BatchNormalization()(x)\n","        return x\n","    return apply"]},{"cell_type":"markdown","metadata":{},"source":["# From Numbers to Text"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def num_to_char_fn(y):\n","    \n","    return [num_to_char.get(x, \"\") for x in y]\n","\n","space_token_idx = CHAR2ORD[' ']\n","\n","@tf.function()\n","def decode_phrase(pred):\n","    x = tf.argmax(pred, axis=1)\n","    diff = tf.not_equal(x[:-1], x[1:])\n","    adjacent_indices = tf.where(diff)[:, 0]\n","    \n","    # Adding the index of the last token\n","    adjacent_indices = tf.concat([adjacent_indices, [tf.size(x) - 1]], 0)\n","    \n","    x = tf.gather(x, adjacent_indices)\n","    mask = x != PAD_TOKEN\n","    x = tf.boolean_mask(x, mask, axis=0)\n","    \n","    if tf.shape(x)[0] < 4:\n","        x = tf.concat([x,tf.convert_to_tensor([CHAR2ORD[c] for c in ' -aero'],dtype=tf.int64)],axis=0)\n","    return x\n","\n","# A utility function to decode the output of the network\n","def decode_batch_predictions(pred):\n","    output_text = []\n","    for result in pred:\n","        result = \"\".join(num_to_char_fn(decode_phrase(result).numpy()))\n","        output_text.append(result)\n","    return output_text\n","\n","# A callback class to output a few transcriptions during training\n","class CallbackEval(tf.keras.callbacks.Callback):\n","    \"\"\"Displays a batch of outputs after every epoch.\"\"\"\n","\n","    def __init__(self, dataset,aux_dataset,model):\n","        super().__init__()\n","        self.dataset = dataset\n","        self.aux_dataset = aux_dataset\n","        self.model = model\n","        self.train_score = []\n","        self.val_score = []\n","    def on_epoch_end(self, epoch: int, logs=None):\n","        if epoch % 5 !=0:\n","            return\n","        start_time = time.time()\n","        d = 0\n","        n = 0\n","        for batch in self.dataset.take(1000//BATCH_SIZE):\n","            X, y = batch\n","            batch_predictions = self.model([X])\n","            batch_predictions = decode_batch_predictions(batch_predictions)\n","            # predictions.extend(batch_predictions)\n","            for i,label in enumerate(y):\n","                label = \"\".join(num_to_char_fn(label.numpy())).replace(pad_token,'')\n","                n += len(label)\n","                d += lev.distance(label, batch_predictions[i])\n","        print('val metric: ', (n-d)/n)\n","        self.val_score.append((n-d)/n)\n","        d = 0\n","        n = 0\n","        for batch in self.aux_dataset.take(1000//BATCH_SIZE):\n","            X, y = batch\n","            batch_predictions = self.model([X])\n","            batch_predictions = decode_batch_predictions(batch_predictions)\n","            # predictions.extend(batch_predictions)\n","            for i,label in enumerate(y):\n","                label = \"\".join(num_to_char_fn(label.numpy())).replace(pad_token,'')\n","                n += len(label)\n","                d += lev.distance(label, batch_predictions[i])\n","        print('train metric: ', (n-d)/n)\n","        self.train_score.append((n-d)/n)\n","        print('eval time: ', time.time() - start_time)"]},{"cell_type":"markdown","metadata":{},"source":["# Record The Best Weight"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class RestoreBestWeightsIfIncrease(tf.keras.callbacks.Callback):\n","    def __init__(self,optimizer,patience=1, restore_threshold=0.1,min_epoch=0):\n","        super(RestoreBestWeightsIfIncrease, self).__init__()\n","        self.optimizer = optimizer\n","        self.best_optimizer_weights = None\n","        self.patience = patience\n","        self.restore_threshold = restore_threshold\n","        # best_weights to store the weights at which the minimum loss occurs.\n","        self.best_weights = None\n","        # best_loss Will keep track of the lowest loss so far.\n","        self.best_loss = np.Inf\n","        # wait Will keep track of the number of epochs the training has waited when loss is no longer minimum.\n","        self.wait = 0\n","        self.min_epoch = min_epoch\n","    def on_epoch_end(self, epoch, logs=None):\n","        current_val_loss = logs.get(\"loss\")\n","        if np.less(current_val_loss, self.best_loss):\n","            self.best_loss = current_val_loss\n","            self.wait = 0\n","            # Record the best weights if current results is better (less).\n","            self.best_weights = self.model.get_weights()\n","            self.best_optimizer_weights = self.optimizer.get_weights()\n","\n","            print('record best loss = ',self.best_loss)\n","        else:\n","            self.wait += 1\n","            if self.wait >= self.patience and epoch > self.min_epoch:\n","                if current_val_loss - self.best_loss > self.restore_threshold:\n","                    self.model.set_weights(self.best_weights)\n","                    self.optimizer.set_weights(self.best_optimizer_weights)\n","                    print(\"\\nRestoring model weights from the end of the best epoch.\")\n","                    self.wait = 0"]},{"cell_type":"markdown","metadata":{},"source":["# Positional Encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def positional_encoding(maxlen, num_hid):\n","    depth = num_hid/2\n","    positions = tf.range(maxlen, dtype = tf.float32)[..., tf.newaxis]\n","    depths = tf.range(depth, dtype = tf.float32)[np.newaxis, :]/depth\n","    angle_rates = tf.math.divide(1, tf.math.pow(tf.cast(10000, tf.float32), depths))\n","    angle_rads = tf.linalg.matmul(positions, angle_rates)\n","    pos_encoding = tf.concat(\n","      [tf.math.sin(angle_rads), tf.math.cos(angle_rads)],\n","      axis=-1)\n","    return pos_encoding"]},{"cell_type":"markdown","metadata":{},"source":["# Non Pad/SOS/EOS Token Accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# TopK accuracy for multi dimensional output\n","class TopKAccuracy(tf.keras.metrics.Metric):\n","    def __init__(self, k, **kwargs):\n","        super(TopKAccuracy, self).__init__(name=f'top{k}acc', **kwargs)\n","        self.top_k_acc = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=k)\n","\n","    def update_state(self, y_true, y_pred, sample_weight=None):\n","        y_true = tf.reshape(y_true, [-1])\n","        y_pred = tf.reshape(y_pred, [-1, N_UNIQUE_CHARACTERS])\n","        character_idxs = tf.where(y_true < N_UNIQUE_CHARACTERS0)\n","        y_true = tf.gather(y_true, character_idxs, axis=0)\n","        y_pred = tf.gather(y_pred, character_idxs, axis=0)\n","        self.top_k_acc.update_state(y_true, y_pred)\n","\n","    def result(self):\n","        return self.top_k_acc.result()\n","    \n","    def reset_state(self):\n","        self.top_k_acc.reset_state()"]},{"cell_type":"markdown","metadata":{},"source":["# Loss Weights"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Create Initial Loss Weights All Set To 1\n","loss_weights = np.ones(N_UNIQUE_CHARACTERS, dtype=np.float32)\n","# Set Loss Weight Of Pad Token To 0\n","loss_weights[PAD_TOKEN] = 0"]},{"cell_type":"markdown","metadata":{},"source":["# Sparse Categorical Crossentropy With Label Smoothing"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# source:: https://stackoverflow.com/questions/60689185/label-smoothing-for-sparse-categorical-crossentropy\n","def scce_with_ls(y_true, y_pred):\n","    # Filter Pad Tokens\n","    idxs = tf.where(y_true != PAD_TOKEN)\n","    y_true = tf.gather_nd(y_true, idxs)\n","    y_pred = tf.gather_nd(y_pred, idxs)\n","    # One Hot Encode Sparsely Encoded Target Sign\n","    y_true = tf.cast(y_true, tf.int32)\n","    y_true = tf.one_hot(y_true, N_UNIQUE_CHARACTERS, axis=1)\n","    # Categorical Crossentropy with native label smoothing support\n","    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred, label_smoothing=0.25, from_logits=True)\n","    loss = tf.math.reduce_mean(loss)\n","    return loss"]},{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"markdown","metadata":{},"source":["## Model Config"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["steps_per_epoch = int((64211 if VAL else 67208)//BATCH_SIZE) + 1\n","\n","\n","data_cfg = {}\n","data_cfg['FRAME_LEN'] = FRAME_LEN\n","data_cfg['INPUT_DIM'] = INPUT_DIM\n","data_cfg['batch_size'] = BATCH_SIZE\n","\n","model_cfg = {}\n","model_cfg[\"num_layers_enc\"] = 6\n","model_cfg[\"encoder_dim\"] = 384\n","model_cfg[\"num_head\"] =  8\n","model_cfg['kernel_size'] = 27 # should back to 13\n","model_cfg[\"decoder_dim\"] = 384 # 256\n","\n","CLASS_NUM= N_UNIQUE_CHARACTERS\n","\n","model_cfg[\"source_maxlen\"] = TEMPORAL_DIM\n","model_cfg[\"target_maxlen\"] = MAX_PHRASE_LENGTH\n","model_cfg[\"num_classes\"] = CLASS_NUM"]},{"cell_type":"markdown","metadata":{},"source":["## Build The Model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_model():\n","    \n","    inp = tf.keras.layers.Input(shape=(TEMPORAL_DIM,INPUT_DIM))\n","    if not MASKING:\n","        x = inp\n","    else:\n","        x = tf.keras.layers.Masking(mask_value=PADDING_MASKING_VALUE)(inp)\n","    # x = tf.keras.layers.Dense(model_cfg[\"encoder_dim\"]*2, activation='swish')(x)\n","    # x = tf.keras.layers.Dense(model_cfg[\"encoder_dim\"])(x)\n","\n","    x = LandmarkEmbedding(model_cfg[\"encoder_dim\"])(x)\n","    # x = tf.keras.layers.BatchNormalization()(x)\n","    # pe = positional_encoding(TEMPORAL_DIM, model_cfg[\"encoder_dim\"])\n","    # x = x + pe\n","    for i in range(model_cfg['num_layers_enc']):\n","        x = ConformerBlock(dim=model_cfg[\"encoder_dim\"], num_heads=model_cfg[\"num_head\"],ksize=model_cfg['kernel_size'])(x)\n","\n","    x = tf.keras.layers.GRU(model_cfg['decoder_dim'],return_sequences=True)(x)\n","    outputs = tf.keras.layers.Dense(model_cfg['num_classes'])(x)\n","    \n","    # Create Tensorflow Model\n","    model = tf.keras.models.Model(inputs=inp, outputs=outputs)\n","    \n","    # Categorical Crossentropy Loss With Label Smoothing\n","    loss = scce_with_ls\n","\n","    # Adam Optimizer\n","    optimizer = tfa.optimizers.RectifiedAdam(sma_threshold=4)\n","    optimizer = tfa.optimizers.Lookahead(optimizer, sync_period=5)\n","\n","    # TopK Metrics\n","    metrics = [\n","        TopKAccuracy(1),\n","        TopKAccuracy(5),\n","    ]\n","    \n","    model.compile(\n","        loss=loss,\n","        optimizer=optimizer,\n","        metrics=metrics,\n","        loss_weights=loss_weights,\n","    )\n","    \n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tf.keras.backend.clear_session()\n","\n","model = get_model()"]},{"cell_type":"markdown","metadata":{},"source":["# Model Architeture"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Plot model summary\n","model.summary(expand_nested=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Plot Model Architecture\n","tf.keras.utils.plot_model(model, show_shapes=True, show_dtype=True, show_layer_names=True, expand_nested=True, show_layer_activations=True)"]},{"cell_type":"markdown","metadata":{},"source":["# Verify No NaN Predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Verify No NaN predictions\n","def verify_no_nan_predictions():\n","    y_pred = model.predict(\n","        val_dataset if VAL else train_dataset,\n","        steps=N_VAL_STEPS_PER_EPOCH if VAL else 100,\n","        verbose=VERBOSE,\n","    )\n","\n","    print(f'# NaN Values In Predictions: {np.isnan(y_pred).sum()}')\n","    \n","    plt.figure(figsize=(15,8))\n","    plt.title(f'Logit Predictions Initialized Model')\n","    pd.Series(y_pred.flatten()).plot(kind='hist', bins=128)\n","    plt.xlabel('Logits')\n","    plt.grid()\n","    plt.show()\n","    \n","verify_no_nan_predictions()"]},{"cell_type":"markdown","metadata":{},"source":["# Learning Rate Scheduler"]},{"cell_type":"markdown","metadata":{},"source":["## Learning Rate Config"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lr_cfg = {}\n","lr_cfg['lr'] =  1e-3\n","lr_cfg['weight_decay'] = 1e-3 #1e-6\n","lr_cfg['epochs'] = 100\n","\n","lr_cfg['optimizer'] = tfa.optimizers.RectifiedAdam\n","lr_cfg['alpha'] = 0.05 # final lr = lr * alpha\n","lr_cfg['finetune_epochs'] = 0\n","lr_cfg['warmup_epochs'] = N_WARMUP_EPOCHS # 0.1*lr_cfg['epochs']\n","initial_learning_rate = lr_cfg['lr']\n","\n","\n","lr_cfg['scheduler'] = OneCycleLR\n","\n","lr_schedule = lr_cfg['scheduler'](\n","    lr = lr_cfg['lr'],\n","    epochs = lr_cfg['epochs'] + lr_cfg['finetune_epochs'],\n","    steps_per_epoch = steps_per_epoch,\n","    steps_per_update = 1,\n","    resume_epoch = 0,\n","    decay_epochs = lr_cfg['epochs'] - lr_cfg['warmup_epochs'],\n","    sustain_epochs = 0,\n","    warmup_epochs = lr_cfg['warmup_epochs'],\n","    finetune_steps = 0,\n","    finetune_lr = 0.05 * lr_cfg['lr'],\n","    lr_start = lr_cfg['lr']*0.01,\n","    lr_min = lr_cfg['lr']*lr_cfg['alpha'],\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Plote Learning Schedule"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def lrfn(current_step, num_warmup_steps, lr_max, num_cycles=0.50, num_training_steps=N_EPOCHS):\n","    \n","    if current_step < num_warmup_steps:\n","        if WARMUP_METHOD == 'log':\n","            return lr_max * 0.10 ** (num_warmup_steps - current_step)\n","        else:\n","            return lr_max * 2 ** -(num_warmup_steps - current_step)\n","    else:\n","        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n","\n","        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr_max"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def plot_lr_schedule(lr_schedule, epochs):\n","    fig = plt.figure(figsize=(20, 10))\n","    plt.plot([None] + lr_schedule + [None])\n","    # X Labels\n","    x = np.arange(1, epochs + 1)\n","    x_axis_labels = [i if epochs <= 40 or i % 5 == 0 or i == 1 else None for i in range(1, epochs + 1)]\n","    plt.xlim([1, epochs])\n","    plt.xticks(x, x_axis_labels) # set tick step to 1 and let x axis start at 1\n","    \n","    # Increase y-limit for better readability\n","    plt.ylim([0, max(lr_schedule) * 1.1])\n","    \n","    # Title\n","    schedule_info = f'start: {lr_schedule[0]:.1E}, max: {max(lr_schedule):.1E}, final: {lr_schedule[-1]:.1E}'\n","    plt.title(f'Step Learning Rate Schedule, {schedule_info}', size=18, pad=12)\n","    \n","    # Plot Learning Rates\n","    for x, val in enumerate(lr_schedule):\n","        if epochs <= 40 or x % 5 == 0 or x is epochs - 1:\n","            if x < len(lr_schedule) - 1:\n","                if lr_schedule[x - 1] < val:\n","                    ha = 'right'\n","                else:\n","                    ha = 'left'\n","            elif x == 0:\n","                ha = 'right'\n","            else:\n","                ha = 'left'\n","            plt.plot(x + 1, val, 'o', color='black');\n","            offset_y = (max(lr_schedule) - min(lr_schedule)) * 0.02\n","            plt.annotate(f'{val:.1E}', xy=(x + 1, val + offset_y), size=12, ha=ha)\n","    \n","    plt.xlabel('Epoch', size=16, labelpad=5)\n","    plt.ylabel('Learning Rate', size=16, labelpad=5)\n","    plt.grid()\n","    plt.show()\n","\n","# Learning rate for encoder\n","LR_SCHEDULE = [lrfn(step, num_warmup_steps=N_WARMUP_EPOCHS, lr_max=LR_MAX, num_cycles=0.50) for step in range(lr_cfg['epochs'])]\n","# Plot Learning Rate Schedule\n","plot_lr_schedule(LR_SCHEDULE, epochs=lr_cfg['epochs'])\n","# Learning Rate Callback\n","lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda step: LR_SCHEDULE[step], verbose=0)"]},{"cell_type":"markdown","metadata":{},"source":["# Weight Decay Callback"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Custom callback to update weight decay with learning rate\n","class WeightDecayCallback(tf.keras.callbacks.Callback):\n","    def __init__(self, wd_ratio=WD_RATIO):\n","        self.step_counter = 0\n","        self.wd_ratio = wd_ratio\n","    \n","    def on_epoch_begin(self, epoch, logs=None):\n","        model.optimizer.weight_decay = model.optimizer.learning_rate * self.wd_ratio\n","        print(f'learning rate: {model.optimizer.learning_rate.numpy():.2e}, weight decay: {model.optimizer.weight_decay.numpy():.2e}')"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluate Initialized Model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Evaluate Initialized Model On Validation Data\n","y_pred = model.evaluate(\n","    val_dataset if VAL else train_dataset,\n","    steps=N_VAL_STEPS_PER_EPOCH if VAL else TRAIN_STEPS_PER_EPOCH,\n","    verbose=VERBOSE,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["## log parameters during training to your Netpune project"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from tensorflow.keras.callbacks import Callback\n","\n","class CustomNeptuneCallback(Callback):\n","    def __init__(self, run, num_epochs, batch_size, learning_rate, weight_decay_ratio, use_val_set, num_warmup_epochs):\n","        self.run = run\n","        self.num_epochs = num_epochs\n","        self.batch_size = batch_size\n","        self.learning_rate = learning_rate\n","        self.weight_decay_ratio = weight_decay_ratio\n","        self.use_val_set = use_val_set\n","        self.num_warmup_epochs= num_warmup_epochs\n","        self.logged_params = False\n","\n","    def on_epoch_begin(self, epoch, logs=None):\n","        # Log parameters only once at the beginning of training\n","        if not self.logged_params:\n","            self.run[\"parameters/num_epochs\"] = self.params['epochs']  # Total number of epochs\n","            self.run[\"parameters/batch_size\"] = self.batch_size\n","            self.run[\"parameters/maximum_learning_rate\"] = self.learning_rate\n","            self.run[\"parameters/weight_decay_ratio\"] = self.weight_decay_ratio\n","            self.run[\"parameters/use_val_set\"] = self.use_val_set\n","            self.run[\"parameters/num_warmup_epochs\"] = self.num_warmup_epochs\n","            self.logged_params = True\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        logs = logs or {}\n","        self.run[\"training/epoch\"].log(epoch)\n","        for key, value in logs.items():\n","            self.run[f\"training/{key}\"].log(value)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Create a callback that saves the model's weights\n","class CustomModelCheckpoint(tf.keras.callbacks.Callback):\n","    def __init__(self,model):\n","        super().__init__()\n","        self.model = model\n","    def on_epoch_end(self, epoch, logs=None):\n","        if epoch >= 350 and (epoch + 1) % 10 == 0:  # Check if this is the 5th epoch (or multiple thereof)\n","            self.model.save_weights(f'model_weights_{epoch + 1:02d}.h5')"]},{"cell_type":"markdown","metadata":{},"source":["## Group Callbacks Together"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_cb = tf.keras.callbacks.ModelCheckpoint(\n","    'best.h5',\n","    monitor = 'val_loss' if VAL else 'loss',\n","    verbose = 1,\n","    save_best_only = True,\n","    save_weights_only= True,\n","    mode = 'auto',\n","    save_freq='epoch',\n","    options=None,\n","    initial_value_threshold=None,\n",")\n","\n","optimizer = lr_cfg['optimizer'](lr_schedule, weight_decay = lr_cfg['weight_decay'],) # clipnorm = 5.0)\n","optimizer = tfa.optimizers.Lookahead(optimizer, sync_period=5)\n","restore_cb = RestoreBestWeightsIfIncrease(optimizer,patience=1, restore_threshold=1.0,min_epoch=10)\n","epochwise_checkpoint = CustomModelCheckpoint(model)\n","callbacks = [lr_logger,model_cb, restore_cb,epochwise_checkpoint]"]},{"cell_type":"markdown","metadata":{},"source":["## Train Model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if TRAIN_MODEL:\n","    # Clear all models in GPU\n","    tf.keras.backend.clear_session()\n","\n","    # Get new fresh model\n","    model = get_model()\n","    \n","    # Sanity Check\n","    model.summary()\n","\n","    model.build((None,TEMPORAL_DIM,INPUT_DIM))\n","\n","    #Calculate the time taken for train the model\n","    start_time=time.time()\n","\n","    # Actual Training\n","    history = model.fit(\n","            x=train_dataset,\n","            steps_per_epoch=steps_per_epoch,\n","            epochs=lr_cfg['epochs'],\n","            # Only used for validation data since training data is a generator\n","            validation_data=val_dataset if VAL else None,\n","            validation_steps=0 if not VAL else -(1000//-BATCH_SIZE),\n","            callbacks=[\n","                lr_callback,\n","                WeightDecayCallback(),\n","                callbacks,\n","                CustomNeptuneCallback(\n","                    run=run,\n","                    num_epochs=lr_cfg['epochs'],\n","                    batch_size=data_cfg['batch_size'],\n","                    learning_rate=lr_cfg['lr'],\n","                    weight_decay_ratio=lr_cfg['weight_decay'],\n","                    use_val_set=VAL,\n","                    num_warmup_epochs= lr_cfg['warmup_epochs']\n","                )\n","            ],\n","            verbose=VERBOSE,\n","        )\n","        \n","    end_time=time.time()\n","    \n","    model.save_weights('model.h5')"]},{"cell_type":"markdown","metadata":{},"source":["## Save Model Weights"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Save Model Weights\n","model.save_weights('ASL-19.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Log the saved model\n","run[\"artifacts/model\"].upload('ASL-19.h5')"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluate Model on Validation Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Verify Model is Loaded Correctly\n","model.evaluate(\n","    val_dataset if VAL else train_dataset,\n","    steps=N_VAL_STEPS_PER_EPOCH if VAL else TRAIN_STEPS_PER_EPOCH,\n","    batch_size=BATCH_SIZE,\n","    verbose=VERBOSE,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# Inference with TFLiteModel to save the model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Model Layer Names\n","for l in model.layers:\n","    print(l.name)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class TFLiteModel(tf.Module):\n","    def __init__(\n","        self,\n","        model,\n","    ):\n","        super(TFLiteModel, self).__init__()\n","        self.model = model\n","\n","    @tf.function(\n","        input_signature=[\n","            tf.TensorSpec(shape=[None, len(SEL_COLS)], dtype=tf.float32, name=\"inputs\")\n","        ]\n","    )\n","    def __call__(self, inputs, training=False):\n","        # Preprocess Data\n","        x = tf.cast(inputs, tf.float32)\n","        x = x[None]\n","        x = tf.cond(\n","            tf.shape(x)[1] == 0,\n","            lambda: tf.zeros((1, 1, len(SEL_COLS))),\n","            lambda: tf.identity(x),\n","        )\n","        x = x[0]\n","        x = preprocess1(x)\n","        x = preprocess2(x)\n","        x = x[None]\n","        x = self.model(x)\n","        x = x[0]\n","        x = decode_phrase(x)\n","        x = tf.cond(tf.shape(x)[0] == 0, lambda: tf.zeros(1, tf.int64), lambda: tf.identity(x))\n","        x = tf.one_hot(x, 59)\n","        return {\"outputs\": x}\n","\n","tflitemodel_base = TFLiteModel(model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["keras_model_converter = tf.lite.TFLiteConverter.from_keras_model(tflitemodel_base)\n","# before\n","keras_model_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n","# keras_model_converter.optimizations = [tf.lite.Optimize.DEFAULT] # comment for speed\n","\n","# now\n","keras_model_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","keras_model_converter.target_spec.supported_types = [tf.float16]\n","\n","tflite_model = keras_model_converter.convert()\n","with open(\"model.tflite\", \"wb\") as f:\n","    f.write(tflite_model)\n","\n","infargs = {\"selected_columns\": SEL_COLS}\n","\n","with open(\"inference_args.json\", \"w\") as json_file:\n","    json.dump(infargs, json_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import zipfile\n","\n","# list of file names to be zipped\n","file_names = [\"model.tflite\", \"inference_args.json\"]\n","\n","# name of the zip file\n","zip_name = \"submission.zip\"\n","\n","# create a ZipFile object\n","with zipfile.ZipFile(zip_name, 'w') as zipf:\n","    # write each file into the zip file\n","    for file in file_names:\n","        zipf.write(file)\n","\n","print(f'{zip_name} file is created.')"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":5973250,"sourceId":52950,"sourceType":"competition"},{"datasetId":3354626,"sourceId":5835808,"sourceType":"datasetVersion"},{"datasetId":3282038,"sourceId":5939194,"sourceType":"datasetVersion"},{"datasetId":3531756,"sourceId":6156851,"sourceType":"datasetVersion"},{"sourceId":139787577,"sourceType":"kernelVersion"}],"dockerImageVersionId":30498,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
